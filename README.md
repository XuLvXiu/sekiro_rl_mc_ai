
## 动机与成效

九月份，我们训练了一个图片分类模型来再战稀世强者苇名弦一郎，模型准确率不高，快速起身与喝血瓶都需要人工的协助，但运气好的时候也能打过一阶段。

十月份，在图片分类模型的基础上，我们尝试了强化学习中比较简单的蒙特卡洛方法(MC, monte carlo methods)，

最终训练出来的 policy，具有了一定的快速起身与喝血瓶的能力（有时候也起不来，有时候喝血瓶也挨揍），

它比分类模型喝更少的血瓶就可以击败弦一郎的第一阶段，但也有较大的机率在角落里面卡掉视角而死亡，

这个 policy 稳定性非常的强，绝大部分时间都在防御，只在擒拿危出现的时候，才会使用两连斩来打断 boss 并造成伤害。

稳定性带来的代价就是效率的下降，打完一阶段得二十分钟左右，稳扎稳打一点都不刺激。

总之，这个 policy 比分类模型需要更少的人工协助即可以完成目标。

当然，policy 是否成功并不是特别的重要，更重要的是在项目过程中，我们通过实践对 MC mthods 的原理与应用有了更为清晰的理解。


## 演示视频

三周目白金的蒙特卡洛狼 再战 稀世强者弦一郎

https://www.bilibili.com/video//

## 游戏设置

- steam 开始游戏
- 游戏设置：图像设定 -- 屏幕模式设置为窗口，游戏分辨率调整为1280*720
- 游戏设置：按键设置 -- ‘使用道具‘ 设置为按键 p, 动作（长按）吸引设置为按键 e. 重置视角/固定目标设置为 q，跳跃键为 f(未用到)，垫步/识破键为空格，鼠标左键攻击，右键防御
- 把 '葫芦' 设置为第一个快捷使用的道具, 最好只设置这一个道具
- 游戏设置：网络设定 -- 标题画面启动设定为 离线游玩.  目的是为了关闭残影，满地的残影烦死了。
- 保持游戏窗口在最上层，不要最小化或者被其它的全屏窗口覆盖。

## 思路与问题

众所周知，MC methods 仅适用于回合制任务，所以一些科普类的文章喜欢以 21 点游戏来进行举例。

只狼作为一个动作游戏，它其实也可以操作成一个回合制游戏，boss 出一招，player 反制一招，如此循环，就相当于回合制了。

状态 state 的定义是一个比较关键的问题。

直观来讲，可以把 boss 当前正在使用的招式作为当前的状态，针对 boss 不同的招式状态，player 作出相应的应对，这应该也是人类新手玩家的打法。

但是在游戏过程中，通过程序很难识别出 boss 当前处于哪种招式，或许这种方式确实是可行的，但我们未能成功做到。


退而求其次，我们把部分游戏区域的截图作为 state, 此截图也就是图片分类项目中的截图，大小为 301x301，本意是想做成 300x300 的，不想多写了一个像素。

此方法有一个致命的问题，状态空间是极其巨大的，即使做了灰度化处理，每个像素点的取值也有 256 种可能，而一个状态截图中这样的像素点有 300x300 = 90K 个。

那么，状态空间有多大呢，我们一开始犯了一个计算上的严重错误，错误的认为状态空间是 256 * 90K， 但其实是 256 的 90K 次方。

面对如此巨大的状态空间，即使是把 300x300 压缩成 10x10，也没有太大的意义，在游戏过程中很难很难遇到相同的 state， MC methods 毫无用武之地。

经过一番调研，我们使用原始的 resnet 模型，从截图中提取特征，每张截图都产生了 512 维度的特征，但是特征本身是浮点数，仍然是无比巨大的状态空间，

然后使用 kmeans 对截图的特征进行 36 个分类的聚类，这样就把状态空间限制到了 36。 在此基础上应用 MC methods，最终训练出来的 policy 特别喜欢使用垫步。

用垫步的方法来打弦一郎，除非是高手中的高手，不然只有死路一条。回想我本人第一次玩只狼的时候，用垫步五次就过了蝴蝶夫人，三次过了骑马的鬼形部，于是信心爆棚觉得只狼的 boss 也不过如此贴吧的吧友们都太菜了，然后遇到弦一郎被虐了几十次连二阶段都进不去，才不得不在痛苦与迷茫中放弃了垫步的打法，改为一招一式的防御反击。

后来，换成图片分类项目中的 resnet 模型，kmeans 8 个分类的聚类，最终训练出来了一个特别喜欢防御的 policy，它甚至全程都在防御，从不出手攻击。可想而知，如果把这样的 policy 放到股票市场中，它可能会为了避免亏损而永不开仓。

再后来，我们在动作空间中加入了一些攻击的连招，比如垫步+攻击、垫步+防御+攻击、攻击+攻击，在 state-7(各种危) 中终于可以对 boss 造成伤害了。

在训练的过程中，还会遇到一个问题，state-7 里面全是各种危，它在游戏中出现的概率非常低，会导致此 state 的训练量不足，尤其是随机选中的 action 极有可能使得游戏在极短时间内就结束，危的出现概率就更低了。所以在 state 0 ~ 6 训练的差不多的时候，就改了改代码，把 state 0 ~ 6 的随机性取消，只选择最优的 argmax 动作，这样游戏时间就会显著延长，危出现的概率就变大了。由此可见，epsilon 应该是 state 相关的，不应全部 state 共享同一个 epsilon.

reward 的设计也非常的重要，这直接影响到了 policy 的打法，我们这个 policy 的打法是: 

`立足防御，找机会偷一刀，尽量少垫步，绝不贪刀，稳扎稳打`

最后，设计了两个新的 state：8 与 9。  state-8 指的是 player 受到了伤害； state-9 指的是 boss 受到了伤害且 player hp < 60，此时应该去喝血瓶。

从最终结果来看，这种状态定义方式也并不见得有多合理，state 未能完整反映出游戏中的真实情况。


## 如何训练

- 确认环境

`python debug_display_game_info.py`

会在 assets 目录中生成 debug_ui_elements.jpg，该图片中会绘制游戏屏幕截图中的各个 window 区域

同时还会弹出一个小窗口实时显示 player 与 boss 的 hp. 这个功能得感谢原作者。


- 收集数据

`python data_collector.py`

按 ] 键开始或暂停收集; 

所谓的收集，就是在分类模型与 MC policy 自动打游戏的过程中，定期对游戏屏幕的敌兵区域进行截屏(301x301)，以 list 的形式保存在内存中。

一个 episode 结束的时候，内存中的数据集会被保存到硬盘文件中。

按 Backspace 键，退出程序

如果在命令行中使用了 `--new` 参数，会首先清除 images 目录中的全部文件

也可以不必收集，把图片分类项目中收集的图片文件 copy 到 images 目录中也可以。


- 训练聚类模型

`python cluster_model.py` 

会结合图片分类项目中训练出来的 resnet 模型与 kmeans，训练出一个8分类的聚类模型 model.cluster.kmeans

resnet 负责提取 images 目录中各个图片的特征，kmeans 对这些特征进行聚类，最后会把各个类的图片copy一份到 images/1 image/2 等子目录中。

在我们这次训练的过程中，images/7 里面是大量的各种危的图片，images/2 里面也有少量的危。


- 训练 MC policy

`python train.py`

默认会加载 checkpoint 文件中的训练相关信息以及Q和N，然后在此基础上进行训练。

如果在命令行中使用了 `--new` 参数，会从第0个 episode 开始重新训练。

每一个 episode 结束之后，更新Q与N 并保存到 checkpoint 文件中。


- 查看 Q与 N

`python checkpoint.py`


- 测试：执行某个或者某几个动作

`python test.py`


## 预测

进入游戏，

在 cmd 窗口中运行：
```
python main.py 
```

等待模型加载完，

按 q 键锁定敌方

按 ] 键, 就会针对敌方的出招自动做出预测动作了。

再次按下 ] 键，会停止预测。

按 Backspace 键，退出程序。


## 人工备份

模型的训练结果主要涉及到如下的几个文件：
- images	            收集到的截屏图像文件
- checkpoint.json		记录了当前是哪个episode，以及完成训练时的时间。
- checkpoint.pkl		存储了 Q 与 N
- model.cluster.kmeans  聚类模型
- model.resnet.v1  图片分类项目的训练成果


如果要训练新模型的话，可能需要对老模型的这些数据进行备份。


## 大部分代码和思路来自以下网址，感谢他们

- https://github.com/XR-stb/DQN_WUKONG
- https://github.com/analoganddigital/DQN_play_sekiro
- https://github.com/Sentdex/pygta5
- https://github.com/RongKaiWeskerMA/sekiro_play
- https://www.lapis.cafe/posts/ai-and-deep-learning/%E4%BD%BF%E7%94%A8resnet%E8%AE%AD%E7%BB%83%E4%B8%80%E4%B8%AA%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B
- https://blog.csdn.net/qq_36795658/article/details/100533639
- https://blog.csdn.net/Guo_Python/article/details/134922730

